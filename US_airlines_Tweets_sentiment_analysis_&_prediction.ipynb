{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "US_airlines_Tweets_sentiment analysis & prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RishiNandhan/US-airlines-tweets-sentiment-analysis-prediction/blob/master/US_airlines_Tweets_sentiment_analysis_%26_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfIx5jWDrfBS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "6b974eed-c932-4464-fe17-5fc2901b612b"
      },
      "source": [
        "#import all the necessary packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCLXPz5YrfG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#set the display option to display all rows and columns\n",
        "pd.set_option(\"display.max_rows\",None)\n",
        "pd.set_option(\"display.max_columns\",None)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y283h81rfMt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "de4312cd-4b05-40e9-fced-497e246240fe"
      },
      "source": [
        "#Load the data\n",
        "data=pd.read_csv(\"Tweets.csv\")\n",
        "print(data.info())  #check for the info\n",
        "print(data.shape)   #get the dimensions of the data\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 14640 entries, 0 to 14639\n",
            "Data columns (total 15 columns):\n",
            "tweet_id                        14640 non-null int64\n",
            "airline_sentiment               14640 non-null object\n",
            "airline_sentiment_confidence    14640 non-null float64\n",
            "negativereason                  9178 non-null object\n",
            "negativereason_confidence       10522 non-null float64\n",
            "airline                         14640 non-null object\n",
            "airline_sentiment_gold          40 non-null object\n",
            "name                            14640 non-null object\n",
            "negativereason_gold             32 non-null object\n",
            "retweet_count                   14640 non-null int64\n",
            "text                            14640 non-null object\n",
            "tweet_coord                     1019 non-null object\n",
            "tweet_created                   14640 non-null object\n",
            "tweet_location                  9907 non-null object\n",
            "user_timezone                   9820 non-null object\n",
            "dtypes: float64(2), int64(2), object(11)\n",
            "memory usage: 1.7+ MB\n",
            "None\n",
            "(14640, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrS2JHFPrfRF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "58d02f79-7cde-4b42-eb67-034bcf3bced0"
      },
      "source": [
        "#get the unique airline counts\n",
        "airline_count=data['airline'].value_counts().sort_values(ascending=False)\n",
        "print(airline_count)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "United            3822\n",
            "US Airways        2913\n",
            "American          2759\n",
            "Southwest         2420\n",
            "Delta             2222\n",
            "Virgin America     504\n",
            "Name: airline, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-XDQhKfrfTt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "08fe8bc8-c6b0-482a-9081-4866298611d4"
      },
      "source": [
        "#percentage of tweets in each airline\n",
        "tweet_percentage=(data[['airline','text']].groupby('airline').count()/len(data)).sort_values(by='text',ascending=False)\n",
        "print(tweet_percentage)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                    text\n",
            "airline                 \n",
            "United          0.261066\n",
            "US Airways      0.198975\n",
            "American        0.188456\n",
            "Southwest       0.165301\n",
            "Delta           0.151776\n",
            "Virgin America  0.034426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjJkYHxWrfbA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "388c2461-2037-4f56-e27a-8d8b047c9770"
      },
      "source": [
        "#tweets count for each airline and airline_sentiment\n",
        "print(pd.crosstab(data.airline,data.airline_sentiment))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "airline_sentiment  negative  neutral  positive\n",
            "airline                                       \n",
            "American               1960      463       336\n",
            "Delta                   955      723       544\n",
            "Southwest              1186      664       570\n",
            "US Airways             2263      381       269\n",
            "United                 2633      697       492\n",
            "Virgin America          181      171       152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m49AOiCFrfd_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "b30bc672-8336-4d9b-ba73-7d3f10892374"
      },
      "source": [
        "#create a seperate df for tweets\n",
        "tweets=data['text'].values\n",
        "tweets_df=pd.DataFrame(data={'tweets':tweets},index=np.arange(len(tweets)))\n",
        "print(tweets_df.head())\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              tweets\n",
            "0                @VirginAmerica What @dhepburn said.\n",
            "1  @VirginAmerica plus you've added commercials t...\n",
            "2  @VirginAmerica I didn't today... Must mean I n...\n",
            "3  @VirginAmerica it's really aggressive to blast...\n",
            "4  @VirginAmerica and it's a really big bad thing...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuPYPxvhrfj8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "c9b8ddd6-2457-48e4-c17b-1a96a7e9bfd6"
      },
      "source": [
        "#get the no of words in each tweet\n",
        "tweets_df['no_of_words']=tweets_df['tweets'].apply(lambda x: len(str(x).split()))\n",
        "print(tweets_df.head())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              tweets  no_of_words\n",
            "0                @VirginAmerica What @dhepburn said.            4\n",
            "1  @VirginAmerica plus you've added commercials t...            9\n",
            "2  @VirginAmerica I didn't today... Must mean I n...           12\n",
            "3  @VirginAmerica it's really aggressive to blast...           17\n",
            "4  @VirginAmerica and it's a really big bad thing...           10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_oDMTwarfpT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "135377c7-f7f9-49f6-b4ec-7905f9ac6779"
      },
      "source": [
        "#get the no of characters in each tweet\n",
        "tweets_df['no_of_chars']=tweets_df['tweets'].apply(lambda x: len(str(x)))\n",
        "print(tweets_df.head())\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              tweets  no_of_words  no_of_chars\n",
            "0                @VirginAmerica What @dhepburn said.            4           35\n",
            "1  @VirginAmerica plus you've added commercials t...            9           72\n",
            "2  @VirginAmerica I didn't today... Must mean I n...           12           71\n",
            "3  @VirginAmerica it's really aggressive to blast...           17          126\n",
            "4  @VirginAmerica and it's a really big bad thing...           10           55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZDtg1nwrfr1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "e042ec44-0f93-408c-afda-676a23ed7840"
      },
      "source": [
        "#get the avg word length in each tweet\n",
        "tweets_df['avg_word_length']=tweets_df['no_of_chars']/tweets_df['no_of_words']\n",
        "print(tweets_df.head())\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              tweets  no_of_words  \\\n",
            "0                @VirginAmerica What @dhepburn said.            4   \n",
            "1  @VirginAmerica plus you've added commercials t...            9   \n",
            "2  @VirginAmerica I didn't today... Must mean I n...           12   \n",
            "3  @VirginAmerica it's really aggressive to blast...           17   \n",
            "4  @VirginAmerica and it's a really big bad thing...           10   \n",
            "\n",
            "   no_of_chars  avg_word_length  \n",
            "0           35         8.750000  \n",
            "1           72         8.000000  \n",
            "2           71         5.916667  \n",
            "3          126         7.411765  \n",
            "4           55         5.500000  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRmkxMB7rfwl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b269ade0-a807-4618-f917-888057befed8"
      },
      "source": [
        "#get the stop words from NLTK package\n",
        "stop=stopwords.words('english')\n",
        "print(len(stop))\n",
        "print(stop)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtaFjZxnrfzN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "cd0edc9d-ea1c-4082-d8fd-a013bf54d5f9"
      },
      "source": [
        "#get the no of stop words in each tweet\n",
        "tweets_df['no_of_stopwords']=tweets_df['tweets'].apply(lambda x: len([x for x in str(x).split() if x in stop]) )\n",
        "print(tweets_df.head())\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              tweets  no_of_words  \\\n",
            "0                @VirginAmerica What @dhepburn said.            4   \n",
            "1  @VirginAmerica plus you've added commercials t...            9   \n",
            "2  @VirginAmerica I didn't today... Must mean I n...           12   \n",
            "3  @VirginAmerica it's really aggressive to blast...           17   \n",
            "4  @VirginAmerica and it's a really big bad thing...           10   \n",
            "\n",
            "   no_of_chars  avg_word_length  no_of_stopwords  \n",
            "0           35         8.750000                0  \n",
            "1           72         8.000000                3  \n",
            "2           71         5.916667                2  \n",
            "3          126         7.411765                6  \n",
            "4           55         5.500000                5  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glqbTiExrf1n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "95057407-c4f6-42d6-8027-bbdca01cafe9"
      },
      "source": [
        "#get the no of numeric in each tweets\n",
        "tweets_df['no_of_numeric']=tweets_df['tweets'].apply(lambda x: len([x for x in str(x).split() if x.isnumeric()]))\n",
        "print(tweets_df.head())\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              tweets  no_of_words  \\\n",
            "0                @VirginAmerica What @dhepburn said.            4   \n",
            "1  @VirginAmerica plus you've added commercials t...            9   \n",
            "2  @VirginAmerica I didn't today... Must mean I n...           12   \n",
            "3  @VirginAmerica it's really aggressive to blast...           17   \n",
            "4  @VirginAmerica and it's a really big bad thing...           10   \n",
            "\n",
            "   no_of_chars  avg_word_length  no_of_stopwords  no_of_numeric  \n",
            "0           35         8.750000                0              0  \n",
            "1           72         8.000000                3              0  \n",
            "2           71         5.916667                2              0  \n",
            "3          126         7.411765                6              0  \n",
            "4           55         5.500000                5              0  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm1v2IFMrf8x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "058d4320-de3f-4b79-d817-ec1727600637"
      },
      "source": [
        "#get the no of hashtags in each tweet\n",
        "tweets_df['no_of_hashtag']=tweets_df['tweets'].apply(lambda x: len([x for x in str(x).split() if x.startswith(\"#\")]))\n",
        "print(tweets_df.head())\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              tweets  no_of_words  \\\n",
            "0                @VirginAmerica What @dhepburn said.            4   \n",
            "1  @VirginAmerica plus you've added commercials t...            9   \n",
            "2  @VirginAmerica I didn't today... Must mean I n...           12   \n",
            "3  @VirginAmerica it's really aggressive to blast...           17   \n",
            "4  @VirginAmerica and it's a really big bad thing...           10   \n",
            "\n",
            "   no_of_chars  avg_word_length  no_of_stopwords  no_of_numeric  no_of_hashtag  \n",
            "0           35         8.750000                0              0              0  \n",
            "1           72         8.000000                3              0              0  \n",
            "2           71         5.916667                2              0              0  \n",
            "3          126         7.411765                6              0              0  \n",
            "4           55         5.500000                5              0              0  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6q4zzF6rf_i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "35262687-f112-45b2-fdb4-f44b60d63a44"
      },
      "source": [
        "#remove stop words\n",
        "tweets_df['tweets_cleaned']=tweets_df['tweets'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "print(tweets_df.head())\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              tweets  no_of_words  \\\n",
            "0                @VirginAmerica What @dhepburn said.            4   \n",
            "1  @VirginAmerica plus you've added commercials t...            9   \n",
            "2  @VirginAmerica I didn't today... Must mean I n...           12   \n",
            "3  @VirginAmerica it's really aggressive to blast...           17   \n",
            "4  @VirginAmerica and it's a really big bad thing...           10   \n",
            "\n",
            "   no_of_chars  avg_word_length  no_of_stopwords  no_of_numeric  \\\n",
            "0           35         8.750000                0              0   \n",
            "1           72         8.000000                3              0   \n",
            "2           71         5.916667                2              0   \n",
            "3          126         7.411765                6              0   \n",
            "4           55         5.500000                5              0   \n",
            "\n",
            "   no_of_hashtag                                     tweets_cleaned  \n",
            "0              0                @VirginAmerica What @dhepburn said.  \n",
            "1              0  @VirginAmerica plus added commercials experien...  \n",
            "2              0  @VirginAmerica I today... Must mean I need tak...  \n",
            "3              0  @VirginAmerica really aggressive blast obnoxio...  \n",
            "4              0                @VirginAmerica really big bad thing  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkxmHR3JrgGQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "b7937b52-a04a-4c8c-c8fc-9bcea60ee2ff"
      },
      "source": [
        "#change the tweets to lowercase\n",
        "tweets_df['tweets_cleaned']=tweets_df['tweets_cleaned'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "print(tweets_df.head())\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              tweets  no_of_words  \\\n",
            "0                @VirginAmerica What @dhepburn said.            4   \n",
            "1  @VirginAmerica plus you've added commercials t...            9   \n",
            "2  @VirginAmerica I didn't today... Must mean I n...           12   \n",
            "3  @VirginAmerica it's really aggressive to blast...           17   \n",
            "4  @VirginAmerica and it's a really big bad thing...           10   \n",
            "\n",
            "   no_of_chars  avg_word_length  no_of_stopwords  no_of_numeric  \\\n",
            "0           35         8.750000                0              0   \n",
            "1           72         8.000000                3              0   \n",
            "2           71         5.916667                2              0   \n",
            "3          126         7.411765                6              0   \n",
            "4           55         5.500000                5              0   \n",
            "\n",
            "   no_of_hashtag                                     tweets_cleaned  \n",
            "0              0                @virginamerica what @dhepburn said.  \n",
            "1              0  @virginamerica plus added commercials experien...  \n",
            "2              0  @virginamerica i today... must mean i need tak...  \n",
            "3              0  @virginamerica really aggressive blast obnoxio...  \n",
            "4              0                @virginamerica really big bad thing  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dcsA0ohrgD9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "b1ceb8a4-17f5-4dd3-b20c-0f8ef19a505e"
      },
      "source": [
        "#remove puntuations\n",
        "tweets_df['tweets_cleaned']=tweets_df['tweets_cleaned'].apply(lambda x: x.translate(str.maketrans(\"\",\"\",string.punctuation)))\n",
        "print(tweets_df.head())\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              tweets  no_of_words  \\\n",
            "0                @VirginAmerica What @dhepburn said.            4   \n",
            "1  @VirginAmerica plus you've added commercials t...            9   \n",
            "2  @VirginAmerica I didn't today... Must mean I n...           12   \n",
            "3  @VirginAmerica it's really aggressive to blast...           17   \n",
            "4  @VirginAmerica and it's a really big bad thing...           10   \n",
            "\n",
            "   no_of_chars  avg_word_length  no_of_stopwords  no_of_numeric  \\\n",
            "0           35         8.750000                0              0   \n",
            "1           72         8.000000                3              0   \n",
            "2           71         5.916667                2              0   \n",
            "3          126         7.411765                6              0   \n",
            "4           55         5.500000                5              0   \n",
            "\n",
            "   no_of_hashtag                                     tweets_cleaned  \n",
            "0              0                   virginamerica what dhepburn said  \n",
            "1              0  virginamerica plus added commercials experienc...  \n",
            "2              0  virginamerica i today must mean i need take an...  \n",
            "3              0  virginamerica really aggressive blast obnoxiou...  \n",
            "4              0                 virginamerica really big bad thing  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwTDyPP4rgCX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "98c4647d-5425-4be0-9b22-42b22081f231"
      },
      "source": [
        "# count each words in the dataset\n",
        "count_of_each_word=pd.Series(\" \".join(tweets_df['tweets_cleaned']).split()).value_counts()\n",
        "print(count_of_each_word[:20])\n",
        "#remove top commonly used words\n",
        "remove_common_words=count_of_each_word[:7]\n",
        "tweets_df['tweets_cleaned']=tweets_df['tweets_cleaned'].apply(lambda x: \" \".join(x for x in x.split() if x not in remove_common_words))\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i               5075\n",
            "united          4117\n",
            "flight          3870\n",
            "usairways       3039\n",
            "americanair     2938\n",
            "southwestair    2441\n",
            "jetblue         2256\n",
            "get             1334\n",
            "thanks          1071\n",
            "cancelled       1056\n",
            "service          953\n",
            "you              873\n",
            "help             852\n",
            "time             769\n",
            "customer         746\n",
            "im               740\n",
            "us               678\n",
            "hours            669\n",
            "flights          645\n",
            "2                644\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzJtC947rf6P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "6396c1bc-2c0a-4936-d101-d895b516bd74"
      },
      "source": [
        "#remove rarely used words\n",
        "print(count_of_each_word[-20:])\n",
        "remove_rare_words=count_of_each_word[-20:]\n",
        "tweets_df['tweets_cleaned']=tweets_df['tweets_cleaned'].apply(lambda x: \" \".join(x for x in x.split() if x not in remove_rare_words))\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lotbut               1\n",
            "s4                   1\n",
            "psgrs                1\n",
            "n615jb               1\n",
            "cheatcustomers       1\n",
            "viraltech            1\n",
            "dimensions           1\n",
            "enforcement          1\n",
            "nycbuenos            1\n",
            "seanmfmadden         1\n",
            "bad👎👎                1\n",
            "herself              1\n",
            "httptco5txu5tsfkj    1\n",
            "ils                  1\n",
            "angriest             1\n",
            "princesses           1\n",
            "powered              1\n",
            "compact              1\n",
            "diet                 1\n",
            "23oct                1\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg2-SdD7rf4m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "ece5fadd-064e-4ac7-c79a-ee3acaaa9f90"
      },
      "source": [
        "#Lemantize each word\n",
        "tweets_df['tweets_cleaned']=tweets_df['tweets_cleaned'].apply(lambda x: \" \".join([Word(x).lemmatize() for x in x.split()]))\n",
        "print(tweets_df.head())\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              tweets  no_of_words  \\\n",
            "0                @VirginAmerica What @dhepburn said.            4   \n",
            "1  @VirginAmerica plus you've added commercials t...            9   \n",
            "2  @VirginAmerica I didn't today... Must mean I n...           12   \n",
            "3  @VirginAmerica it's really aggressive to blast...           17   \n",
            "4  @VirginAmerica and it's a really big bad thing...           10   \n",
            "\n",
            "   no_of_chars  avg_word_length  no_of_stopwords  no_of_numeric  \\\n",
            "0           35         8.750000                0              0   \n",
            "1           72         8.000000                3              0   \n",
            "2           71         5.916667                2              0   \n",
            "3          126         7.411765                6              0   \n",
            "4           55         5.500000                5              0   \n",
            "\n",
            "   no_of_hashtag                                     tweets_cleaned  \n",
            "0              0                   virginamerica what dhepburn said  \n",
            "1              0  virginamerica plus added commercial experience...  \n",
            "2              0  virginamerica today must mean need take anothe...  \n",
            "3              0  virginamerica really aggressive blast obnoxiou...  \n",
            "4              0                 virginamerica really big bad thing  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWzXRJTRuCTK",
        "colab_type": "text"
      },
      "source": [
        "Spell check requires lot of computation power and it is very time consuming. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcCp5MgRrfug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#spell check\n",
        "tweets_df['tweets_cleaned']=tweets_df['tweets_cleaned'].apply(lambda x: str(TextBlob(x).correct()))\n",
        "print(tweets_df.head())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xhy5JQIrfnx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#change to Tf-Idf vectorization\n",
        "vectorizer=TfidfVectorizer(max_features=3000,min_df=7,max_df=0.8)\n",
        "processed_features=vectorizer.fit_transform(tweets_df['tweets_cleaned']).toarray()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zJGV9D4rfh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#split the data into train and test\n",
        "x_train, x_test, y_train, y_test=train_test_split(processed_features,data['airline_sentiment'],test_size=0.2,random_state=21)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QCtJjtjrfgn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "4d2b2fb3-e909-4d6f-e7bf-e9b9c71f112c"
      },
      "source": [
        "#Random Forest Classifier\n",
        "text_classifier = RandomForestClassifier(n_estimators=200, random_state=21)\n",
        "text_classifier.fit(x_train, y_train)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
              "                       n_jobs=-1, oob_score=False, random_state=21, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1a08xyFrfYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#make prediction using Random Forest\n",
        "predictions=text_classifier.predict(x_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_JAQA72shHr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "e48124c6-50a0-4ddb-f8ed-fd3da1e2a4a4"
      },
      "source": [
        "#evaluate using metrics\n",
        "print(\"\\n CONFUSION MATRIX \\n\",confusion_matrix(y_test,predictions))\n",
        "print(\"\\n CLASSIFICATION REPORT\\n\",classification_report(y_test,predictions))\n",
        "print(\"\\n ACCURACY SCORE:\",accuracy_score(y_test, predictions))\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " CONFUSION MATRIX \n",
            " [[1682  101   47]\n",
            " [ 270  301   61]\n",
            " [ 140   64  262]]\n",
            "\n",
            " CLASSIFICATION REPORT\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.80      0.92      0.86      1830\n",
            "     neutral       0.65      0.48      0.55       632\n",
            "    positive       0.71      0.56      0.63       466\n",
            "\n",
            "    accuracy                           0.77      2928\n",
            "   macro avg       0.72      0.65      0.68      2928\n",
            "weighted avg       0.75      0.77      0.75      2928\n",
            "\n",
            "\n",
            " ACCURACY SCORE: 0.7667349726775956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT_HRnkushKo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72b23899-c8f9-4c06-bc4d-19df1baa8e68"
      },
      "source": [
        "#Gaussian NaiveBayes Classifier\n",
        "NB=GaussianNB()\n",
        "NB.fit(x_train,y_train)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcIgkNCfrfWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#make prediction using Gaussian NaiveBayes Classifier\n",
        "nb_prediction=NB.predict(x_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqe0ijBKslJb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "1dc4622d-4324-4f42-d70e-f6ae200d3f3c"
      },
      "source": [
        "print(\"\\n CONFUSION MATRIX \\n\",confusion_matrix(y_test,nb_prediction))\n",
        "print(\"\\n CLASSIFICATION REPORT\\n\",classification_report(y_test,nb_prediction))\n",
        "print(\"\\n ACCURACY SCORE:\",accuracy_score(y_test, nb_prediction))\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " CONFUSION MATRIX \n",
            " [[635 467 728]\n",
            " [ 50 198 384]\n",
            " [ 37  67 362]]\n",
            "\n",
            " CLASSIFICATION REPORT\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.88      0.35      0.50      1830\n",
            "     neutral       0.27      0.31      0.29       632\n",
            "    positive       0.25      0.78      0.37       466\n",
            "\n",
            "    accuracy                           0.41      2928\n",
            "   macro avg       0.47      0.48      0.39      2928\n",
            "weighted avg       0.65      0.41      0.43      2928\n",
            "\n",
            "\n",
            " ACCURACY SCORE: 0.40812841530054644\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}